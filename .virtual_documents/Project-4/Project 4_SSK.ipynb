








# Import libraries
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split
sns.set_style("darkgrid")
from sklearn.model_selection import train_test_split


# Load nhanes data
nhanes = pd.read_csv('nhanes.csv')
# Get the ID numbers for each observation (seqn)
seqn = nhanes['SEQN']
# Get the target, "self-reported health condition," HSD010
hsd010 = nhanes['HSD010']
# Drop SEQN from the dataframe and then apply the standard scaler
nhanes = nhanes.drop(['SEQN', 'HSD010'], axis = 1)
nhanes_scaled = pd.DataFrame(StandardScaler().fit_transform(nhanes),
                             columns = nhanes.columns)
# Add the ID and target back in
nhanes_scaled['SEQN'] = seqn
nhanes_scaled['HSD010'] = hsd010
nhanes_scaled = nhanes_scaled.set_index('SEQN')
nhanes_scaled.head()








# Create a binary version of hsd010 where 1-3 are "good" and 4-5 are "poor"
nhanes_scaled['HSD010_binary'] = hsd010_binary = nhanes_scaled['HSD010'].replace(
    [1, 2, 3, 4, 5], ['good', 'good', 'good', 'poor', 'poor']) 
# Recode the original hsd010 with the string labels
nhanes_scaled['HSD010'] = nhanes_scaled['HSD010'].replace(
    [1, 2, 3, 4, 5], ['excellent', 'very good', 'good', 'fair', 'poor'])
# Boxplot of hsd010
ax = sns.boxplot(x = 'HSD010', y = 'INDFMPIR', data = nhanes_scaled)
ax.set(xlabel = "Self Reported Health Condition",
      ylabel = "Family Income to Poverty Line Ratio")
ax.set_title("Boxplot of Family Income to Poverty Line Ratio by Self-Reported Health Status")
plt.show()


# Boxplot of hsd010_binary
ax = sns.boxplot(x = 'HSD010_binary', y = 'INDFMPIR', data = nhanes_scaled)
ax.set(xlabel = "Self Reported Health Condition",
      ylabel = "Family Income to Poverty Line Ratio")
ax.set_title("Boxplot of Family Income to Poverty Line Ratio by Binary Self-Reported Health Status")
plt.show()





ax = sns.scatterplot(x = "INDFMPIR", y = "BMXBMI", hue = "HSD010", palette = "tab10", data = nhanes_scaled)
ax.set(xlabel = "Income to Poverty Line Ratio",
      ylabel = "Body Mass Index")
ax.set_title("BMI v. Income-Poverty Ratio")
plt.show()





nhanes_scaled = nhanes_scaled.drop(['HSD010', 'HSD010_binary'], axis = 1)














# Fit PCA on full data 
# ----------
pca_all = PCA() 
principalComponents_nhanes = pca_all.fit_transform(nhanes_scaled)

# Create scree plot 
# ----------
PC_values = np.arange(pca_all.n_components_) + 1
cumulative_variance_ratio = np.cumsum(pca_all.explained_variance_ratio_)

plt.plot(PC_values, pca_all.explained_variance_ratio_, 'o-', linewidth=2, color='blue')
plt.plot(PC_values, cumulative_variance_ratio, 'ro-', linewidth=2, color='red')
plt.title('Scree Plot with All Components')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.legend(['Individual Variance Ratio', 'Cumulative Variance Ratio'])

ax = plt.gca()
ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_ylabel('Variance Explained (%)')
ax2.set_yticklabels(['{:,.0%}'.format(x) for x in ax2.get_yticks()])

plt.show()





# Train PCA with 100 components
# ----------
pca_100 = PCA(n_components=100)
principalComponents_100 = pca_100.fit_transform(nhanes_scaled)

# Create scree plot
# ----------
PC_values_100 = np.arange(pca_100.n_components_) + 1
cumulative_variance_ratio = np.cumsum(pca_100.explained_variance_ratio_)

plt.plot(PC_values_100, pca_100.explained_variance_ratio_, 'o-', linewidth=2, color='blue')
plt.plot(PC_values_100, cumulative_variance_ratio, 'ro-', linewidth=2, color='red')
plt.title('Scree Plot with 100 Components')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.legend(['Individual Variance Ratio', 'Cumulative Variance Ratio'])

ax = plt.gca()
ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_ylabel('Variance Explained (%)')
ax2.set_yticklabels(['{:,.0%}'.format(x) for x in ax2.get_yticks()])

plt.show()





# Train PCA with 50 components
# ----------
pca_50 = PCA(n_components=50)
principalComponents_50 = pca_50.fit_transform(nhanes_scaled)

# Create scree plot 
# ----------
PC_values_50 = np.arange(pca_50.n_components_) + 1
cumulative_variance_ratio = np.cumsum(pca_50.explained_variance_ratio_)

plt.plot(PC_values_50, pca_50.explained_variance_ratio_, 'o-', linewidth=2, color='blue')
plt.plot(PC_values_50, cumulative_variance_ratio, 'ro-', linewidth=2, color='red')
plt.title('Scree Plot with 50 Components')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.legend(['Individual Variance Ratio', 'Cumulative Variance Ratio'])

ax = plt.gca()
ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_ylabel('Variance Explained (%)')
ax2.set_yticklabels(['{:,.0%}'.format(x) for x in ax2.get_yticks()])

plt.show()





# Barplot of variation explained by 100 components 
# ----------
plt.bar(PC_values_100, pca_100.explained_variance_ratio_)
plt.xlabel('Principal Component')
plt.ylabel('Proportion of Variance Explained')
plt.title('Barplot of Variance Explained by 100 Components')


# Barplot of variation explained by 50 components 
# ----------
plt.bar(PC_values_50, pca_50.explained_variance_ratio_)
plt.xlabel('Principal Component')
plt.ylabel('Proportion of Variance Explained')
plt.title('Barplot of Variance Explained by 50 Components')











# Turn PCA with 50 components into dataframe
# ----------
pca_50_df = pd.DataFrame(data = principalComponents_50)


# Create 2D plot
# ----------
ax = sns.scatterplot(x = 0, y = 1, data = pca_50_df)
ax.set(xlabel = "Principal Component - 1",
      ylabel = "Principal Component - 2")
plt.show()

















# set random seed
np.random.seed(28)

k_values = range(1, 21)

# calculate inertia for different values of k
inertia = []
for k in k_values:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(nhanes_scaled)
    inertia.append(kmeans.inertia_)

# plot the elbow method using inertia_
plt.figure(figsize=(10, 6))
plt.plot(k_values, inertia, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.xticks(k_values)
plt.grid()
plt.show()


# Set random seed
# ----------
np.random.seed(28)

# Use the inertia_ attribute to create the elbow plot
# ----------
inertia = []
k_values = range(1, 21)
for k in k_values:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(nhanes_scaled)
    inertia.append(kmeans.inertia_)

# Plot the elbow method to choose k
plt.figure(figsize=(10, 6))
plt.plot(k_values, inertia, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k using inertia_')
plt.xticks(k_values)
plt.grid()
plt.show()








# Specify kmeans algorithm
# ----------
kmeans = KMeans(n_clusters=4,
                n_init=10,
                max_iter=300 
                ).fit(nhanes_scaled)


# Adding cluster labels to dataframe
# ----------
nhanes_scaled['clusters'] = kmeans.labels_





# Plot clusters on scatterplot
# ----------
ax = sns.scatterplot(x = "INDFMPIR", y = "BMXBMI", hue = "clusters", palette = "viridis", data = nhanes_scaled)
ax.set(xlabel = "Income to Poverty Line Ratio",
      ylabel = "Body Mass Index")
ax.set_title("BMI v. Income-Poverty Ratio")
# ax.legend(title='Clusters', labels=['1', '2', '3', '4'])
plt.show()








kmeans_PCA = KMeans(n_clusters=4,
                n_init=10,
                max_iter=300 
                ).fit(principalComponents_50)


pca_50_df['PCAclusters'] = kmeans_PCA.labels_


ax = sns.scatterplot(x = 0, y = 1, hue = "PCAclusters", palette = "viridis", data = pca_50_df)
ax.set(xlabel = "Principal Component - 1",
      ylabel = "Principal Component - 2")
plt.show()

















# partition data
# -----------
y = ... # either hsd010 or hsd010_binary, may need to convert to numeric if it isn't already 
X = ... # drop out any columns that aren't features

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size = .25, 
                                                    random_state = 10)


# load libraries
# -----------
import keras
from keras.utils import to_categorical

num_classes = ...
# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(..., ...)
y_test = keras.utils.to_categorical(..., ...)
num_classes = y_test.shape[1]


# create neural network model
# -----------
model = Sequential()

model.add(Dense(..., input_dim= ..., kernel_initializer= ..., activation= ...))

model.add(Dense(..., kernel_initializer= ..., activation= ...))

## Add any additional layers you wish here

model.compile(loss= ..., optimizer= ..., metrics=[...])

model.fit(..., ..., validation_data=(..., ...), epochs=..., batch_size=..., verbose=...)





## Your Answer Here





## Your Answer Here


















